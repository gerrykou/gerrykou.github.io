---
layout: post
title: How to enable the AI assistant in marimo notebooks
lead: 
---

Iâ€™ve just published a step-by-step guide on how to enable the AI assistant in marimo notebooks using a local LLM model with Docker Model Runner, but is the same process if you are running your LLMs locally with Ollama.
I have also included a video on how to do the setup.

Running an LLM locally allows you to build with privacy and for free, giving you greater control over your projects and your data.

Find more [here](https://github.com/gerrykou/marimo_local_ai_assistant)

